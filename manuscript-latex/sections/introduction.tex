\section{Introduction}
\label{sec:introduction}

This essay establishes on the paper entitled \textit{"On deep learning as a remedy for the curse dimensionality in nonparametric regression"} by \cite{Bauer2019}. In that paper, it is proved that least squares estimates constructed from multilayer feedforward neural networks are able to circumvent the curse of dimensionality in nonparametric regression as long as a smoothness condition and a suitable restriction on the structure of regression function hold. 

Contrary to parametric approach, the regression function in nonparametric approach is not claimed to be characterized by finitely many parameters, so the whole function is data driven. Moreover, as claimed by \cite{Gyoerfi2002}, one has to restrict the class of regression functions when considering the rate of convergence. Thus, \cite{Bauer2019} introduce the definition of $(p,C)$-smoothness. One can simply take $p$ as the degree of smoothness even though they are slightly different. \cite{Stone1982} shows that the optimal minimax rate of convergence for estimating a $(p,C)$-smooth regression function is $n^{-\frac{2p}{2p+d}}$, where $d$ is the dimensionality of input. Apparently, the rate of convergence can be exceedingly small when $d$ is comparatively large to $p$, and this property is the so-called curse of dimensionality.

Much endeavor has been made to bypass this problem. \cite{Stone1985} adds an additivity condition to the structure of regression function, and the resulting model is a sum of $(p,C)$-smooth univariate functions $m_1,\ldots,m_d: \mathbb{R} \rightarrow \mathbb{R}$. \cite{Stone1985} shows that $n^{-\frac{2p}{2p+1}}$ is the optimal minimax rate of convergence in this occasion. \cite{Stone1994} further generalizes it to an interaction model, which allows the existence of interaction terms in smooth functions. He proves that the optimal minimax rate of convergence then becomes $n^{-\frac{2p}{2p+d^{*}}}$ where $d^{*}  \in \{1,\ldots,d\}$ is the number of interaction terms. 

Another stream of research takes account of the so-called single index model, where the $d$ dimensions of input are firstly linearly combined and then transformed by a univariate function (e.g., \cite{Hardle1993}). This idea is extended to the well-known projection pursuit, where the function is a sum of functions of those in single index model (e.g., \cite{Friedman1981}). It is demonstrated that if the univariate functions in above models are $(p,C)$-smooth, the regression estimates can reach the univariate rates of convergence (i.e., $n^{-\frac{2p}{2p+1}}$) up to some logarithmic factor (e.g., \cite{Gyoerfi2002}). Furthermore, \cite{Horowitz2007} also consider a regression model where the innermost layer is a sum of smooth functions, each of which takes a single component of $d$ dimensional inputs as its own input, and the output of inner layer is then recursively used as input of outer layer. They show a univariate rate of $n^{-\frac{2p}{2p+1}}$ in such model with the use of a penalized least squares estimate for smoothing splines.

\cite{Bauer2019} argue that, even though the above models can achieve a satisfactory rate of convergence, it is only possible with the imposed strict assumptions being satisfied. Thus, they aim to derive rates of convergence for more general forms of functions, which approximate the real world better and are hopefully simpler. In their paper, they propose a model of multilayer feed-forward neural networks, which not only cleverly imitate the property of modularity in real-world complex technical systems, but also utilize the advantage of using neural networks in high dimensional occasions. \cite{Bauer2019} contributes to the existing literature body by proving an achievable rate of convergence irrespective of $d$ in their model and meanwhile allowing $p$ in $(p,C)$-smooth regression functions to be larger than 1. This greatly relaxes the assumptions of precedent models.

The rest of essay is organized as follows. Section \ref{sec:review} reviews the statistical model and some important theorems from the paper. In Section \ref{sec:simulation}, Monte-Carlo simulations are carried out and in Section \ref{sec:shiny}, an R Shiny app is designed to interactively visualize the performance of the model and other alternatives. Section \ref{sec:conclusion} concludes this essay.